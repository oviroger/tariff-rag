# ğŸ“‹ GuÃ­a de AnotaciÃ³n de Retrieval

## ğŸ¯ Objetivo

Evaluar la calidad del sistema de recuperaciÃ³n (retrieval) marcando si cada documento recuperado es **relevante** o **no relevante** para clasificar el producto descrito en la query.

---

## ğŸ“Š Estado Actual

- **Total de registros:** 500 (100 queries Ã— 5 docs cada una)
- **Anotados:** 0 (0%)
- **Pendientes:** 500 (100%)

---

## âš™ï¸ MÃ©todos de AnotaciÃ³n

### OpciÃ³n 1: Herramienta Interactiva (Recomendada)

```powershell
python evaluation/tools/annotate_retrieval.py --csv evaluation/templates/eval_retrieval_asgard.csv
```

**Ventajas:**
- âœ… Interfaz interactiva con progreso en tiempo real
- âœ… Muestra query + snippet juntos
- âœ… Guarda progreso automÃ¡ticamente
- âœ… Permite volver atrÃ¡s (back), saltar (skip), y retomar despuÃ©s
- âœ… EstadÃ­sticas en vivo

**Controles:**
- `1` = Relevante
- `0` = No relevante
- `s` = Skip (dejar para despuÃ©s)
- `b` = Volver al anterior
- `q` = Guardar y salir
- `?` = Ver ayuda

**Continuar anotaciÃ³n:**
```powershell
# Si interrumpes, puedes retomar desde donde dejaste:
python evaluation/tools/annotate_retrieval.py --csv evaluation/templates/eval_retrieval_asgard.csv --start-from 150
```

---

### OpciÃ³n 2: Excel/LibreOffice (Manual)

```powershell
# Generar versiÃ³n simplificada
python evaluation/tools/simplify_retrieval_csv.py `
  --input evaluation/templates/eval_retrieval_asgard.csv `
  --output evaluation/templates/eval_retrieval_asgard_simple.csv
```

Luego:
1. Abre `eval_retrieval_asgard_simple.csv` en Excel
2. Llena la columna **relevance** con `0` o `1`
3. Guarda el archivo
4. Copia los valores de vuelta a `eval_retrieval_asgard.csv`

**Ventajas:**
- âœ… Puedes filtrar, buscar, usar formulas
- âœ… Anotar en bloques
- âœ… Copiar/pegar valores

**Desventajas:**
- âŒ No hay validaciÃ³n automÃ¡tica
- âŒ MÃ¡s propenso a errores de formato

---

## ğŸ§­ Criterios de Relevancia

### âœ… Un documento es RELEVANTE (1) si:

- Contiene informaciÃ³n que **ayudarÃ­a a clasificar correctamente** el producto
- Menciona el **capÃ­tulo HS correcto** o productos similares
- Describe **caracterÃ­sticas, materiales o usos** relacionados con la query
- Proporciona **contexto Ãºtil** para la clasificaciÃ³n arancelaria
- Es del mismo **grupo/familia de productos**

### âŒ Un documento es NO RELEVANTE (0) si:

- Habla de **productos completamente diferentes**
- Menciona **capÃ­tulos HS no relacionados**
- Es **texto genÃ©rico** sin valor para la clasificaciÃ³n
- Contiene informaciÃ³n **contradictoria o confusa**
- Es **ruido del OCR** o texto sin sentido

---

## ğŸ’¡ Criterio PrÃ¡ctico

> **Pregunta clave:** Si fueras un agente de aduana clasificando el producto de la query,  
> Â¿este fragmento te ayudarÃ­a a asignar el cÃ³digo HS correcto?
> 
> - **SÃ** â†’ Marca como `1` (relevante)
> - **NO** â†’ Marca como `0` (no relevante)

---

## ğŸ“ Ejemplos de AnotaciÃ³n

### Ejemplo 1: RELEVANTE âœ…

**Query:**  
`YARA PERLADA FERTILIZANTE SACOS UREA PARA USO AGRICOLA`

**Ground Truth HS:** `3102.10` (CapÃ­tulo 31: Fertilizantes)

**Snippet Recuperado:**  
`UREA. Las demÃ¡s, incluidas las mezclas no comprendidas en las subpartidas precedentes.`

**Relevancia:** `1` (Relevante)  
**RazÃ³n:** Menciona directamente "UREA" y pertenece al capÃ­tulo correcto.

---

### Ejemplo 2: NO RELEVANTE âŒ

**Query:**  
`YARA PERLADA FERTILIZANTE SACOS UREA PARA USO AGRICOLA`

**Ground Truth HS:** `3102.10` (CapÃ­tulo 31: Fertilizantes)

**Snippet Recuperado:**  
`Papel y cartÃ³n, ondulados, incluso perforados.`

**Relevancia:** `0` (No relevante)  
**RazÃ³n:** Habla de papel y cartÃ³n, completamente diferente a fertilizantes.

---

### Ejemplo 3: PARCIALMENTE RELEVANTE â†’ RELEVANTE âœ…

**Query:**  
`TUBO DE ACERO REDONDO NEGRO VELAN TUBO DE ACERO REDONDO 22 MM`

**Ground Truth HS:** `7306.30` (CapÃ­tulo 73: Tubos de acero)

**Snippet Recuperado:**  
`Los demÃ¡s tubos (por ejemplo: soldados o remachados) de secciÃ³n circular...`

**Relevancia:** `1` (Relevante)  
**RazÃ³n:** Aunque no menciona "redondo negro" especÃ­ficamente, describe tubos de secciÃ³n circular, que es relevante para la clasificaciÃ³n.

---

## ğŸ” Verificar Progreso

```powershell
python evaluation/eval_retrieval_annotated.py --csv evaluation/templates/eval_retrieval_asgard.csv --verbose
```

Esto mostrarÃ¡:
- Total de registros
- Anotados / Pendientes
- Porcentaje de completitud
- DistribuciÃ³n (relevantes vs no relevantes)

---

## ğŸ¯ Calcular MÃ©tricas (DespuÃ©s de Anotar)

Una vez completada la anotaciÃ³n (o con anotaciÃ³n parcial):

```powershell
python evaluation/eval_retrieval_annotated.py --csv evaluation/templates/eval_retrieval_asgard.csv --verbose
```

**MÃ©tricas calculadas:**
- `recall@1, @3, @5`: Â¿Hay al menos 1 doc relevante en top-k?
- `precision@1, @3, @5`: ProporciÃ³n de docs relevantes en top-k
- `ndcg@1, @3, @5`: Ranking quality (penaliza docs relevantes en posiciones bajas)
- `map`: Mean Average Precision (mÃ©trica agregada)

---

## â±ï¸ EstimaciÃ³n de Tiempo

- **Con herramienta interactiva:** ~3-4 horas (500 registros â‰ˆ 30 seg/registro)
- **Con Excel (manual):** ~4-5 horas (mÃ¡s lento por context switching)

**RecomendaciÃ³n:** Hazlo en sesiones de 1 hora, guarda progreso frecuentemente.

---

## ğŸ“‚ Archivos Generados

```
evaluation/
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ eval_retrieval_asgard.csv           # CSV principal (anotar aquÃ­)
â”‚   â””â”€â”€ eval_retrieval_asgard_simple.csv    # VersiÃ³n simplificada para Excel
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ annotate_retrieval.py               # Herramienta interactiva
â”‚   â””â”€â”€ simplify_retrieval_csv.py           # Generador de versiÃ³n simple
â””â”€â”€ eval_retrieval_annotated.py             # Calculador de mÃ©tricas
```

---

## ğŸ†˜ Problemas Comunes

### "No puedo ver bien los snippets largos"
```powershell
# Usar Excel para ver snippets completos
python evaluation/tools/simplify_retrieval_csv.py --input ... --output ...
```

### "InterrumpÃ­ la anotaciÃ³n, Â¿perdÃ­ mi progreso?"
```powershell
# NO, el progreso se guarda automÃ¡ticamente. ContinÃºa con:
python evaluation/tools/annotate_retrieval.py --csv ... --start-from <Ã­ndice>
```

### "Â¿Puedo anotar solo algunas queries?"
SÃ­, las mÃ©tricas se calcularÃ¡n solo con las queries que tengan anotaciones completas (todos sus 5 docs anotados).

### "Â¿QuÃ© pasa si marco todo como 0?"
Las mÃ©tricas serÃ¡n 0, pero es un resultado vÃ¡lido si realmente ningÃºn documento es relevante (indicarÃ­a problema con el retrieval).

---

## âœ… Checklist de Completitud

- [ ] Revisar primeras 10 queries para familiarizarse con el corpus
- [ ] Anotar al menos 50 queries (250 docs) para mÃ©tricas preliminares
- [ ] Completar las 100 queries (500 docs) para evaluaciÃ³n final
- [ ] Verificar coherencia: revisar queries con 0% relevantes y 100% relevantes
- [ ] Calcular mÃ©tricas finales
- [ ] Generar reporte de retrieval

---

**Â¿Listo para empezar?**

```powershell
python evaluation/tools/annotate_retrieval.py --csv evaluation/templates/eval_retrieval_asgard.csv
```

ğŸš€ Â¡Buena suerte con la anotaciÃ³n!
