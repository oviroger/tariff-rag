# üìã Instrucciones para Anotaci√≥n de Archivos de Evaluaci√≥n

## ‚úÖ Archivos Generados

Se generaron autom√°ticamente **80 queries** de prueba distribuidas en:

- **35 queries predefinidas** (productos comunes de diversos cap√≠tulos HS)
- **10 queries extra√≠das** del corpus OpenSearch
- **35 queries sint√©ticas** (variaciones de las anteriores)

### Archivos CSV de Evaluaci√≥n:

1. **`eval_clasificador_hs6.csv`**: 80 registros (1 por query)
2. **`eval_retrieval.csv`**: 392 registros (80 queries √ó ~5 docs)

---

## üìù Paso 1: Anotar `eval_clasificador_hs6.csv`

### Estructura del archivo:
```csv
query_id,query,pred_hs6_1,pred_hs6_2,pred_hs6_3,true_hs6
1,Smartphone con pantalla OLED,8517.12,8517.13,8517.62,
```

### Tarea:
Llenar la columna **`true_hs6`** con el c√≥digo HS correcto (6 d√≠gitos) para cada query.

### Herramientas de apoyo:
1. **Nomenclatura oficial**: Consultar tablas de la WCO/OMA
2. **Sistema actual**: Comparar con predicciones del modelo (`pred_hs6_1`, `pred_hs6_2`, `pred_hs6_3`)
3. **Corpus local**: Buscar en OpenSearch productos similares

### Ejemplo de anotaci√≥n:
```csv
query_id,query,pred_hs6_1,pred_hs6_2,pred_hs6_3,true_hs6
60,Smartphone con pantalla OLED,8517.12,8517.13,8517.62,8517.12
54,Laptop HP 15 pulgadas,8471.30,8471.41,8471.49,8471.30
59,Caf√© en grano tostado,0901.21,0901.11,0901.22,0901.21
72,Manzanas rojas importadas,0808.10,0808.30,0810.90,0808.10
```

### Criterios de anotaci√≥n:
- ‚úÖ **Usar c√≥digo m√°s espec√≠fico**: Si existe c√≥digo de 6 d√≠gitos, usarlo
- ‚úÖ **Ser consistente**: Productos similares deben tener c√≥digos similares
- ‚ö†Ô∏è **Si hay duda**: Dejar vac√≠o o marcar con "REVISAR"
- ‚ö†Ô∏è **Si no existe en HS**: Usar c√≥digo m√°s cercano y anotar en comentarios

### Tiempo estimado: **2-3 horas** (80 queries)

---

## üìù Paso 2: Anotar `eval_retrieval.csv`

### Estructura del archivo:
```csv
query_id,query,doc_id,rank,score,relevance,snippet
1,Smartphone OLED,9d7a5ed04bb6afc8_p9709,1,10.77,,- LED - LCD - OLED - QLED
```

### Tarea:
Llenar la columna **`relevance`** con:
- **1** = El documento **ES relevante** para responder la query
- **0** = El documento **NO ES relevante** para la query

### Criterios de relevancia:

#### ‚úÖ Relevante (1):
- Contiene el c√≥digo HS correcto del producto
- Describe caracter√≠sticas del producto consultado
- Explica reglas de clasificaci√≥n aplicables
- Proporciona ejemplos del mismo tipo de producto

#### ‚ùå No relevante (0):
- Habla de productos completamente diferentes
- C√≥digo HS de otra categor√≠a
- Informaci√≥n gen√©rica sin relaci√≥n
- Fragmentos sin contexto √∫til

### Ejemplo de anotaci√≥n:
```csv
query_id,query,doc_id,rank,score,relevance,snippet
60,Smartphone con pantalla OLED,abc123_p100,1,15.2,1,"8517.12 - Tel√©fonos m√≥viles con pantalla..."
60,Smartphone con pantalla OLED,def456_p200,2,12.8,1,"Caracter√≠sticas: pantalla OLED, t√°ctil..."
60,Smartphone con pantalla OLED,ghi789_p300,3,8.5,0,"0808.10 - Manzanas frescas..." ‚Üê NO relevante
54,Laptop HP 15 pulgadas,jkl012_p400,1,14.1,1,"8471.30 - M√°quinas autom√°ticas procesamiento datos..."
```

### Estrategia eficiente:
1. **Revisar snippet**: Leer el texto recuperado
2. **Comparar con query**: ¬øHabla del mismo producto/categor√≠a?
3. **Verificar c√≥digo HS**: Si aparece, ¬øcorresponde al producto?
4. **Marcar 1 o 0**: Decisi√≥n binaria simple

### Tiempo estimado: **3-4 horas** (392 registros)

---

## üöÄ Paso 3: Ejecutar Evaluaciones

Una vez anotados los archivos, ejecutar los scripts de evaluaci√≥n:

### 3.1 M√©tricas de Clasificaci√≥n:
```powershell
python evaluation/eval_clasificador.py `
  --csv evaluation/templates/eval_clasificador_hs6.csv
```

**Salida esperada:**
```
üìä M√âTRICAS DE CLASIFICACI√ìN
============================
Accuracy@1: 0.725
Accuracy@3: 0.888
Macro-F1: 0.682
Micro-F1: 0.725
MRR@3: 0.801
ECE (calibraci√≥n): 0.134
```

### 3.2 M√©tricas de Recuperaci√≥n:
```powershell
python evaluation/eval_retrieval.py `
  --csv evaluation/templates/eval_retrieval.csv `
  --k 5
```

**Salida esperada:**
```
üìä M√âTRICAS DE RECUPERACI√ìN (IR)
=================================
Recall@1: 0.562
Recall@3: 0.775
Recall@5: 0.850
nDCG@1: 0.562
nDCG@3: 0.689
nDCG@5: 0.734
```

### 3.3 M√©tricas Operacionales:

#### Primero: Generar tr√°fico de prueba
```powershell
python evaluation/tools/warmup_requests.py `
  --base-url http://localhost:8000 `
  --num-classify 100 `
  --num-health 50 `
  --workers 5
```

#### Luego: Exportar logs operacionales
```powershell
python evaluation/export_logs_operativos.py `
  --metrics-url http://localhost:8000/metrics `
  --output evaluation/templates/logs_operativos.csv
```

#### Finalmente: Evaluar operaciones
```powershell
python evaluation/eval_operativo.py `
  --csv evaluation/templates/logs_operativos.csv
```

**Salida esperada:**
```
üìä M√âTRICAS OPERACIONALES
=========================
Latencia P50: 0.245 s
Latencia P95: 0.789 s
Latencia P99: 1.234 s
Throughput: 24.5 QPM
Error Rate: 0.02 (2.0%)
```

---

## üìä Resumen de Tama√±os para Prototipo Acad√©mico

| Archivo | Queries | Registros | Tiempo Anotaci√≥n | Estado |
|---------|---------|-----------|------------------|--------|
| **eval_clasificador_hs6.csv** | 80 | 80 | 2-3 horas | ‚úÖ Generado |
| **eval_retrieval.csv** | 80 | 392 | 3-4 horas | ‚úÖ Generado |
| **logs_operativos.csv** | - | ~100-200 | Autom√°tico | ‚è≥ Por generar |

**Total:** ~5-7 horas de anotaci√≥n manual

---

## üí° Tips para Anotaci√≥n Eficiente

### Para `eval_clasificador_hs6.csv`:
1. **Agrupar por categor√≠a**: Anotar primero todas las queries de electr√≥nica, luego alimentos, etc.
2. **Usar las predicciones**: Si `pred_hs6_1` parece correcto, verificar y usarlo
3. **Consultar documentaci√≥n**: Tener abierta la nomenclatura HS oficial
4. **Anotar en lotes**: 10-15 queries por sesi√≥n, descansar

### Para `eval_retrieval.csv`:
1. **Leer solo el snippet**: No necesitas abrir documentos completos
2. **Decisi√≥n r√°pida**: Si el snippet habla del producto ‚Üí 1, si no ‚Üí 0
3. **Filtrar por query**: Procesar todas las filas de una query a la vez
4. **Usar b√∫squeda**: Ctrl+F para encontrar c√≥digos HS en el snippet

### Validaci√≥n de calidad:
- **Consistencia inter-anotador**: Si es posible, que 2 personas anoten ~10% del dataset
- **Casos ambiguos**: Documentar queries dif√≠ciles en archivo aparte
- **Revisi√≥n final**: Revisar queries con todas las predicciones incorrectas

---

## üéØ Pr√≥ximos Pasos

1. ‚úÖ **Archivos generados** (COMPLETADO)
2. ‚è≥ **Anotar `eval_clasificador_hs6.csv`** (2-3 horas)
3. ‚è≥ **Anotar `eval_retrieval.csv`** (3-4 horas)
4. ‚è≥ **Generar `logs_operativos.csv`** (autom√°tico)
5. ‚è≥ **Ejecutar evaluaciones** (5 minutos)
6. ‚è≥ **Documentar resultados** en tesis (secci√≥n 3.5)

---

## üìÅ Archivos de Referencia

- **Queries generadas**: `evaluation/test_queries.txt`
- **Metadata**: `evaluation/test_queries_metadata.json`
- **Scripts generadores**:
  - `evaluation/tools/generate_test_queries.py`
  - `evaluation/tools/generate_eval_clasificador.py`
  - `evaluation/tools/generate_eval_retrieval.py`
- **Scripts evaluadores**:
  - `evaluation/eval_clasificador.py`
  - `evaluation/eval_retrieval.py`
  - `evaluation/eval_operativo.py`

---

## ‚ùì Preguntas Frecuentes

### ¬øPuedo agregar m√°s queries?
S√≠, ejecuta de nuevo `generate_test_queries.py` con `--target-total 150` para 150 queries.

### ¬øQu√© hago si no conozco el c√≥digo HS correcto?
Consulta:
1. Base de datos WCO/OMA oficial
2. Sistema actual (predicciones del modelo)
3. Experto en comercio internacional

### ¬øPuedo modificar queries existentes?
S√≠, pero mant√©n la consistencia en el `query_id` entre ambos archivos CSV.

### ¬øC√≥mo s√© si mis anotaciones son correctas?
- Compara con predicciones del modelo (si coinciden, probablemente correcto)
- Valida con documentaci√≥n oficial HS
- Revisa casos donde todas las predicciones son incorrectas

---

**√öltima actualizaci√≥n:** 6 de noviembre de 2025
**Autor:** Sistema de Evaluaci√≥n Autom√°tica - Tariff RAG
