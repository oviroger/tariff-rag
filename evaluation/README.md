# Evaluaci√≥n y m√©tricas (3.5)

Este directorio contiene scripts y plantillas para calcular m√©tricas del clasificador, del componente de recuperaci√≥n (RAG) y m√©tricas operativas.

## üéØ Dataset de Evaluaci√≥n Generado

Se han generado **42 queries de prueba** de alta calidad distribuidas en:
- **35 queries predefinidas** (productos comunes de diversos cap√≠tulos HS)
- **7 queries extra√≠das de MySQL** (tabla asgard con productos reales)

### Archivos CSV de Evaluaci√≥n:
- ‚úÖ **`eval_clasificador_hs6.csv`**: 42 queries con predicciones top-3
- ‚úÖ **`eval_retrieval.csv`**: 220 registros (42 queries √ó ~5 docs recuperados)

> üìã **IMPORTANTE**: Ver `INSTRUCCIONES_ANOTACION.md` para completar las columnas `true_hs6` y `relevance` antes de ejecutar evaluaciones.
> 
> ‚è±Ô∏è **Tiempo de anotaci√≥n estimado**: 3-4 horas (vs 5-7 horas del dataset anterior)
> 
> üìä **Validez**: Dataset optimizado con todas las queries siendo productos reales v√°lidos

## Estructura

- `evaluation/eval_clasificador.py`: acc@1, acc@3, macro/micro-F1 (si scikit-learn est√° disponible), MRR@3 y ECE opcional.
- `evaluation/eval_retrieval.py`: recall@k y nDCG@k (relevancia binaria) para la fusi√≥n RRF.
- `evaluation/eval_operativo.py`: latencias p50/p95/p99, throughput (QPM) y tasa de errores desde logs.
- `evaluation/templates/*.csv`: plantillas con datos generados autom√°ticamente.
- `evaluation/tools/`: scripts para generar queries y CSVs de evaluaci√≥n autom√°ticamente.

> Nota de dependencias: los scripts requieren `pandas` y `numpy`. Para macro/micro-F1 se intentar√° usar `scikit-learn`; si no est√° instalado, se omite F1 y se imprime `null`.

## Uso r√°pido

1) Clasificador (HS6):

```bash
python evaluation/eval_clasificador.py --csv evaluation/templates/eval_clasificador_hs6.csv
```

2) Recuperaci√≥n (RAG):

```bash
python evaluation/eval_retrieval.py --csv evaluation/templates/eval_retrieval.csv --k 5
```

3) Operativas:

```bash
python evaluation/eval_operativo.py --csv evaluation/templates/logs_operativos.csv
```


## Exportar logs operativos desde /metrics

Para exportar logs operativos desde el endpoint de m√©tricas, usa:

```sh
python evaluation/export_logs_operativos.py --url http://localhost:8000/metrics --output evaluation/templates/logs_operativos.csv
```

Puedes cambiar el puerto y la URL seg√∫n tu configuraci√≥n:

```sh
python evaluation/export_logs_operativos.py --url http://localhost:<PUERTO>/metrics --output evaluation/templates/logs_operativos.csv
```

## Generar tr√°fico de prueba (warmup)

Para obtener valores distintos de cero, genera tr√°fico contra la API antes de exportar:

```bash
python evaluation/tools/warmup_requests.py --base-url http://localhost:8000 \
	--health 20 --classify 20 --workers 5 --query "Necesito importar pl√°tanos"
```

Luego exporta filtrando si lo deseas por endpoint/m√©todo:

```bash
python evaluation/export_logs_operativos.py --url http://localhost:8000/metrics \
	--endpoint /classify --method POST --output evaluation/templates/logs_operativos.csv
```

Atajo (Windows PowerShell): warmup + export en un solo paso

```powershell
pwsh -File evaluation/tools/warmup_and_export.ps1 -BaseUrl "http://localhost:8000" `
	-Endpoint "/classify" -Method "POST" -Health 20 -Classify 20 -Workers 5 `
	-Query "Necesito importar pl√°tanos" -Output "evaluation/templates/logs_operativos.csv"
```

## Generar CSVs de evaluaci√≥n autom√°ticamente

### 1. Generar queries de prueba

Primero genera un conjunto de queries desde el corpus:

```bash
python evaluation/tools/generate_test_queries.py \
  --os-host http://localhost:9200 \
  --os-index tariff_fragments \
  --os-samples 200 \
  --synthetic 70 \
  --target-total 100 \
  --output evaluation/test_queries.txt
```

Esto genera:
- `evaluation/test_queries.txt`: lista de queries (una por l√≠nea)
- `evaluation/test_queries_metadata.json`: metadata con c√≥digos HS y fuente

**Dataset actual generado:** 80 queries (10 corpus + 35 predefinidas + 35 sint√©ticas)

### 2. Clasificador (eval_clasificador_hs6.csv)

Consulta `/classify` con queries de prueba y registra predicciones:

```bash
python evaluation/tools/generate_eval_clasificador.py \
  --base-url http://localhost:8000 \
  --queries-file evaluation/test_queries.txt \
  --top-n 3 \
  --output evaluation/templates/eval_clasificador_hs6.csv
```

**Archivo generado:** ‚úÖ 80 queries con predicciones top-3

**‚ö†Ô∏è Importante**: Debes llenar manualmente la columna `true_hs6` con los c√≥digos correctos antes de calcular m√©tricas.

### 3. Retrieval (eval_retrieval.csv)

Ejecuta b√∫squedas h√≠bridas en OpenSearch y registra documentos recuperados:

	```bash
	python evaluation/tools/generate_eval_retrieval.py \
		--os-host https://localhost:9200 \
		--index tariff_docs \
		--queries "Smartphone OLED" "Pl√°tanos" \
		--top-k 5 \
		--output evaluation/templates/eval_retrieval.csv
	```

	**Importante**: Debes llenar manualmente la columna `relevance` (0 o 1) para cada documento antes de calcular recall@k y nDCG@k.


## Formatos CSV

Ver archivos en `evaluation/templates/` para los encabezados esperados y ejemplos comentados en la segunda l√≠nea.
