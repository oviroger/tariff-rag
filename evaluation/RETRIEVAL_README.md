# ğŸ” EvaluaciÃ³n de Retrieval - Quick Start

## âœ… Estado Actual

- âœ… Dataset generado: 500 registros (100 queries Ã— 5 docs)
- âœ… Herramientas de anotaciÃ³n creadas
- âœ… Calculador de mÃ©tricas listo
- â³ AnotaciÃ³n pendiente: 0/500 (0%)

---

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: AnotaciÃ³n Interactiva (Recomendada)

```bash
python evaluation/tools/annotate_retrieval.py --csv evaluation/templates/eval_retrieval_asgard.csv
```

**Controles:**
- `1` = Relevante | `0` = No relevante | `s` = Skip | `q` = Quit

**Para continuar despuÃ©s:**
```bash
python evaluation/tools/annotate_retrieval.py --csv evaluation/templates/eval_retrieval_asgard.csv --start-from 250
```

### OpciÃ³n 2: AnotaciÃ³n en Excel

```bash
# 1. Generar versiÃ³n simple
python evaluation/tools/simplify_retrieval_csv.py \
  --input evaluation/templates/eval_retrieval_asgard.csv \
  --output evaluation/templates/eval_retrieval_asgard_simple.csv

# 2. Abrir eval_retrieval_asgard_simple.csv en Excel
# 3. Llenar columna 'relevance' con 0 o 1
# 4. Guardar archivo
```

---

## ğŸ“Š Verificar Progreso

```bash
python evaluation/eval_retrieval_annotated.py --csv evaluation/templates/eval_retrieval_asgard.csv --verbose
```

**Salida:**
```
================================================================================
ESTADO DE ANOTACIÃ“N
================================================================================
Total de registros:     500
Anotados:               125 (25.0%)
  - Relevantes:         45
  - No relevantes:      80
Pendientes:             375
================================================================================
```

---

## ğŸ¯ Calcular MÃ©tricas (DespuÃ©s de Anotar)

```bash
python evaluation/eval_retrieval_annotated.py --csv evaluation/templates/eval_retrieval_asgard.csv --verbose
```

**MÃ©tricas calculadas:**
```json
{
  "recall@1": 0.45,
  "recall@3": 0.68,
  "recall@5": 0.82,
  "precision@1": 0.45,
  "precision@3": 0.31,
  "precision@5": 0.24,
  "ndcg@1": 0.45,
  "ndcg@3": 0.58,
  "ndcg@5": 0.63,
  "map": 0.52,
  "num_queries": 100,
  "num_annotated": 500
}
```

---

## ğŸ“ Criterio de Relevancia

**Pregunta clave:** Â¿Este snippet ayudarÃ­a a un agente de aduana a clasificar correctamente el producto?

### âœ… RELEVANTE (1):
- Menciona el capÃ­tulo HS correcto
- Describe materiales/caracterÃ­sticas similares
- Proporciona contexto Ãºtil para clasificaciÃ³n

### âŒ NO RELEVANTE (0):
- Habla de productos diferentes
- Texto genÃ©rico sin valor
- CapÃ­tulos HS no relacionados

---

## ğŸ“š DocumentaciÃ³n Completa

Ver: [`GUIA_ANOTACION_RETRIEVAL.md`](./GUIA_ANOTACION_RETRIEVAL.md)

---

## ğŸ¯ Ejemplo de AnotaciÃ³n

**Query 1:** `YARA PERLADA FERTILIZANTE SACOS UREA PARA USO AGRICOLA`  
**Ground Truth:** `3102.10` (CapÃ­tulo 31: Fertilizantes)

| Rank | Snippet | Relevancia |
|------|---------|------------|
| 1 | "UREA. Las demÃ¡s, incluidas las mezclas..." | âœ… 1 |
| 2 | "Uso y aplicaciÃ³n: Para uso agrÃ­cola..." | âœ… 1 |
| 3 | "Papel y cartÃ³n, ondulados..." | âŒ 0 |
| 4 | "Fertilizantes minerales o quÃ­micos..." | âœ… 1 |
| 5 | "Los demÃ¡s abonos..." | âœ… 1 |

**Resultado:** 4/5 relevantes â†’ Retrieval exitoso para esta query

---

## â±ï¸ Tiempo Estimado

- **Total:** 3-4 horas para 500 registros
- **Por registro:** ~25-30 segundos
- **RecomendaciÃ³n:** Sesiones de 1 hora con descansos

---

## ğŸ“‚ Archivos

```
evaluation/
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ eval_retrieval_asgard.csv           # â† ANOTAR AQUÃ
â”‚   â””â”€â”€ eval_retrieval_asgard_simple.csv    # VersiÃ³n para Excel
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ annotate_retrieval.py               # Herramienta interactiva
â”‚   â”œâ”€â”€ simplify_retrieval_csv.py           # Generador versiÃ³n simple
â”‚   â””â”€â”€ generate_eval_retrieval.py          # (ya usado)
â”œâ”€â”€ eval_retrieval_annotated.py             # Calculador de mÃ©tricas
â”œâ”€â”€ GUIA_ANOTACION_RETRIEVAL.md            # GuÃ­a completa
â””â”€â”€ RETRIEVAL_README.md                     # Este archivo
```

---

## ğŸ†˜ Troubleshooting

### Problema: "No veo bien los snippets"
**SoluciÃ³n:** Usa la versiÃ³n Excel:
```bash
python evaluation/tools/simplify_retrieval_csv.py --input ... --output ...
```

### Problema: "InterrumpÃ­ la anotaciÃ³n"
**SoluciÃ³n:** El progreso se guarda automÃ¡ticamente. ContinÃºa con `--start-from <Ã­ndice>`

### Problema: "Algunos snippets son muy cortos"
**SoluciÃ³n:** Esos son artefactos del OCR. MÃ¡rcalos como NO relevantes (0) si no aportan informaciÃ³n Ãºtil.

---

## âœ… Checklist

- [ ] Leer guÃ­a de anotaciÃ³n completa
- [ ] Probar herramienta con primeras 5 queries
- [ ] Anotar al menos 50 queries (250 docs) para mÃ©tricas preliminares
- [ ] Completar 100 queries (500 docs) para evaluaciÃ³n final
- [ ] Calcular mÃ©tricas finales
- [ ] Guardar resultados en `evaluation/results/retrieval_asgard_metrics.json`

---

**Â¿Listo para empezar?** ğŸš€

```bash
python evaluation/tools/annotate_retrieval.py --csv evaluation/templates/eval_retrieval_asgard.csv
```
