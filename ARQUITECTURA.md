# ğŸ“š Arquitectura del Sistema Tariff-RAG

## Ãndice

1. [VisiÃ³n General](#visiÃ³n-general)
2. [Arquitectura de Componentes](#arquitectura-de-componentes)
3. [Flujo de Datos](#flujo-de-datos)
4. [MÃ³dulos del Backend](#mÃ³dulos-del-backend)
5. [ConfiguraciÃ³n](#configuraciÃ³n)
6. [Despliegue](#despliegue)
7. [MÃ©tricas y EvaluaciÃ³n](#mÃ©tricas-y-evaluaciÃ³n)
8. [Troubleshooting](#troubleshooting)

---

## VisiÃ³n General

**Tariff-RAG** es un sistema RAG (Retrieval-Augmented Generation) para clasificaciÃ³n arancelaria que combina:

- ğŸ” **BÃºsqueda HÃ­brida**: SemÃ¡ntica (kNN) + LÃ©xica (BM25)
- ğŸ¤– **LLM**: Google Gemini para generaciÃ³n y embeddings
- ğŸ“„ **OCR**: Azure Form Recognizer o Tesseract
- ğŸ’¾ **Vector DB**: OpenSearch con Ã­ndices kNN
- ğŸ—„ï¸ **Fuente de Datos**: MySQL como fuente adicional de informaciÃ³n para el corpus

**Caso de uso**: Dado un producto (ej: "NeumÃ¡ticos radiales 205/55R16"), el sistema:
1. Busca fragmentos relevantes en normativa arancelaria y casos previos
2. Genera el cÃ³digo HS correcto (ej: 4011.10.00.00)
3. Proporciona evidencia textual y confianza

---

## Arquitectura de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       CAPA DE PRESENTACIÃ“N                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Gradio UI (Puerto 7860)                                    â”‚
â”‚  - Interfaz web interactiva                                 â”‚
â”‚  - Formularios de clasificaciÃ³n                             â”‚
â”‚  - VisualizaciÃ³n de resultados                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CAPA DE APLICACIÃ“N                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FastAPI (Puerto 8000)                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  /classify    â”‚  /query      â”‚  /health /metrics   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚          â”‚              â”‚                   â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        Orquestador RAG (chain_rag.py)              â”‚   â”‚
â”‚  â”‚  - Pipeline de retrieval                           â”‚   â”‚
â”‚  â”‚  - Re-ranking                                      â”‚   â”‚
â”‚  â”‚  - GeneraciÃ³n con LLM                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Retrieval Layer â”‚    â”‚  Generation Layerâ”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
        â”‚  - Embeddings    â”‚    â”‚  - Gemini API    â”‚
        â”‚  - Hybrid Search â”‚    â”‚  - Prompt Eng.   â”‚
        â”‚  - Reranking     â”‚    â”‚  - Parsing       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CAPA DE DATOS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   OpenSearch     â”‚  â”‚    MySQL     â”‚  â”‚ Gemini API   â”‚ â”‚
â”‚  â”‚   (Puerto 9200)  â”‚  â”‚ (Puerto 3306)â”‚  â”‚  (Externo)   â”‚ â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”‚
â”‚  â”‚ â€¢ Fragmentos     â”‚  â”‚ â€¢ Casos      â”‚  â”‚ â€¢ Embeddings â”‚ â”‚
â”‚  â”‚ â€¢ Embeddings     â”‚  â”‚ â€¢ Productos  â”‚  â”‚ â€¢ Generation â”‚ â”‚
â”‚  â”‚ â€¢ Ãndices kNN    â”‚  â”‚ â€¢ HistÃ³rico  â”‚  â”‚              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â–²                      â”‚                            â”‚
â”‚         â”‚                      â”‚ ETL                        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚           IngestiÃ³n desde MySQL â†’ OpenSearch                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Servicios Docker

| Servicio | Imagen | Puerto | FunciÃ³n |
|----------|--------|--------|---------|
| **opensearch** | `opensearchproject/opensearch:2.11.0` | 9200, 9600 | Motor de bÃºsqueda vectorial y lÃ©xica |
| **dashboards** | `opensearchproject/opensearch-dashboards:2.11.0` | 5601 | UI de monitoreo de OpenSearch |
| **mysql** | `mysql:8.0` | 3306 | **Fuente de datos**: casos de clasificaciÃ³n, productos histÃ³ricos |
| **api** | `python:3.11-slim` (custom) | 8000 | Backend FastAPI con lÃ³gica RAG |
| **ui** | `python:3.11-slim` | 7860 | Frontend Gradio |

---

## Flujo de Datos

### 1. Ingesta de Documentos (IndexaciÃ³n)

```
FUENTES DE DATOS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PDF/IMG â”‚    â”‚   MySQL DB  â”‚
â”‚(Normat.)â”‚    â”‚ (HistÃ³rico) â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
     â”‚                â”‚
     â”‚ OCR            â”‚ ETL
     â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Texto   â”‚    â”‚  Texto   â”‚
â”‚  Crudo   â”‚    â”‚ Casos    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚Chunking  â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Embedding â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚OpenSearchâ”‚
        â”‚  Ãndice  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Paso 1a: OCR (ExtracciÃ³n desde PDFs)

**Azure Form Recognizer** (recomendado para producciÃ³n):
```python
# app/ocr/azure_provider.py
from azure.ai.formrecognizer import DocumentAnalysisClient

def extract_pdf(file_path: str) -> List[OCRFragment]:
    client = DocumentAnalysisClient(endpoint=ENDPOINT, credential=KEY)
    
    with open(file_path, "rb") as f:
        poller = client.begin_analyze_document("prebuilt-layout", f)
        result = poller.result()
    
    fragments = []
    for page in result.pages:
        for line in page.lines:
            fragments.append(OCRFragment(
                text=line.content,
                page=page.page_number,
                bbox=(line.polygon[0].x, line.polygon[0].y, 
                      line.polygon[2].x - line.polygon[0].x,
                      line.polygon[2].y - line.polygon[0].y),
                confidence=line.confidence
            ))
    
    return fragments
```

**Tesseract** (alternativa gratuita):
```python
# app/ocr/tesseract_provider.py
import pytesseract
import pypdfium2 as pdfium

def extract_pdf(file_path: str) -> List[OCRFragment]:
    pdf = pdfium.PdfDocument(file_path)
    fragments = []
    
    for page_idx in range(len(pdf)):
        # Renderizar a 300 DPI
        page = pdf.get_page(page_idx)
        bitmap = page.render(scale=300/72.0).to_pil()
        
        # OCR con detecciÃ³n de lÃ­neas
        data = pytesseract.image_to_data(
            bitmap, 
            lang="spa+eng",
            output_type=pytesseract.Output.DICT
        )
        
        # Agrupar por lÃ­neas
        for i, text in enumerate(data["text"]):
            if text.strip():
                fragments.append(OCRFragment(
                    text=text,
                    page=page_idx + 1,
                    bbox=(data["left"][i], data["top"][i],
                          data["width"][i], data["height"][i]),
                    confidence=float(data["conf"][i]) / 100.0
                ))
    
    pdf.close()
    return fragments
```

#### Paso 1b: ETL desde MySQL (Casos HistÃ³ricos)

```python
# app/etl_mysql.py
from typing import List, Optional
from pydantic import BaseModel
import pymysql
from app.config import get_settings

class MySQLFragment(BaseModel):
    """Fragmento extraÃ­do de MySQL."""
    text: str
    fragment_id: str
    metadata: dict

def extract_mysql_fragments(
    table: str = "product_cases",
    text_column: str = "description",
    id_column: str = "id"
) -> List[MySQLFragment]:
    """
    Extrae fragmentos de texto desde MySQL para indexar en OpenSearch.
    
    Caso de uso tÃ­pico: 
    - Tabla 'product_cases' con casos histÃ³ricos de clasificaciÃ³n
    - Tabla 'products' con descripciones de productos
    - Tabla 'rulings' con resoluciones arancelarias
    
    Args:
        table: Nombre de la tabla
        text_column: Columna con el texto a indexar
        id_column: Columna con identificador Ãºnico
    
    Returns:
        Lista de fragmentos para indexar
    """
    settings = get_settings()
    
    # Conectar a MySQL
    connection = pymysql.connect(
        host=settings.mysql_host,
        port=settings.mysql_port,
        user=settings.mysql_user,
        password=settings.mysql_password,
        database=settings.mysql_db,
        charset='utf8mb4'
    )
    
    fragments = []
    
    try:
        with connection.cursor(pymysql.cursors.DictCursor) as cursor:
            # Query para extraer datos relevantes
            # Ajustar segÃºn tu esquema de BD
            query = f"""
                SELECT 
                    {id_column} as id,
                    {text_column} as text,
                    hs_code,
                    product_name,
                    classification_date,
                    confidence_score
                FROM {table}
                WHERE {text_column} IS NOT NULL 
                  AND LENGTH({text_column}) > 50
                ORDER BY classification_date DESC
            """
            
            cursor.execute(query)
            rows = cursor.fetchall()
            
            for row in rows:
                # Construir texto enriquecido
                text_parts = [row['text']]
                
                if row.get('product_name'):
                    text_parts.insert(0, f"Producto: {row['product_name']}")
                
                if row.get('hs_code'):
                    text_parts.append(f"CÃ³digo HS: {row['hs_code']}")
                
                if row.get('confidence_score'):
                    text_parts.append(f"Confianza previa: {row['confidence_score']:.2f}")
                
                full_text = "\n".join(text_parts)
                
                fragments.append(MySQLFragment(
                    text=full_text,
                    fragment_id=f"mysql_{table}_{row['id']}",
                    metadata={
                        "source": "mysql",
                        "table": table,
                        "record_id": row['id'],
                        "hs_code": row.get('hs_code'),
                        "product_name": row.get('product_name'),
                        "classification_date": str(row.get('classification_date')),
                        "confidence_score": row.get('confidence_score'),
                        "bucket": "historico_mysql"
                    }
                ))
        
    finally:
        connection.close()
    
    return fragments


def sync_mysql_to_opensearch(
    tables: List[str] = ["product_cases", "customs_rulings"],
    batch_size: int = 100
):
    """
    Sincroniza mÃºltiples tablas de MySQL a OpenSearch.
    
    Ejemplo:
        sync_mysql_to_opensearch(["product_cases", "customs_rulings"])
    """
    from app.os_ingest import bulk_ingest_fragments
    from app.config import get_settings
    
    settings = get_settings()
    total_indexed = 0
    
    for table in tables:
        print(f"ğŸ“¥ Extrayendo desde tabla: {table}")
        
        fragments = extract_mysql_fragments(table)
        
        if fragments:
            # Convertir a dict para ingestiÃ³n
            fragments_dict = [f.model_dump() for f in fragments]
            
            # Indexar en batches
            for i in range(0, len(fragments_dict), batch_size):
                batch = fragments_dict[i:i+batch_size]
                bulk_ingest_fragments(batch, settings.opensearch_index)
                total_indexed += len(batch)
                print(f"  âœ… Indexados {len(batch)} fragmentos")
        
        print(f"  Total desde {table}: {len(fragments)}")
    
    print(f"\nğŸ‰ Total indexado desde MySQL: {total_indexed} fragmentos")
```

#### Paso 2: Chunking (FragmentaciÃ³n)

```python
# scripts/chunk_and_index.py
def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """
    Divide el texto en fragmentos solapados para preservar contexto.
    
    Args:
        text: Texto completo del documento
        chunk_size: Tokens por fragmento (~500 tokens â‰ˆ 2000 caracteres)
        overlap: Tokens de solapamiento entre fragmentos
    
    Returns:
        Lista de fragmentos de texto
    """
    words = text.split()
    chunks = []
    
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    
    return chunks

# Ejemplo de uso
full_text = "\n".join([f.text for f in ocr_fragments])
chunks = chunk_text(full_text, chunk_size=500, overlap=50)
```

#### Paso 3: GeneraciÃ³n de Embeddings

```python
# app/embedder_gemini.py
import google.generativeai as genai

class GeminiEmbedder:
    def __init__(self):
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        self.model = "models/embedding-001"  # 768 dimensiones
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """
        Genera embeddings para una lista de textos.
        
        Returns:
            Lista de vectores de 768 dimensiones
        """
        embeddings = []
        
        # Procesar en batches de 100 (lÃ­mite de API)
        for i in range(0, len(texts), 100):
            batch = texts[i:i+100]
            
            for text in batch:
                result = genai.embed_content(
                    content=text,
                    model=self.model,
                    task_type="retrieval_document"
                )
                embeddings.append(result["embedding"])
        
        return embeddings
```

#### Paso 4: IndexaciÃ³n en OpenSearch

```python
# app/os_ingest.py
from opensearchpy import OpenSearch, helpers

def bulk_ingest_fragments(fragments: List[dict], index_name: str):
    """
    Indexa fragmentos en OpenSearch usando bulk API.
    
    Acepta fragmentos de mÃºltiples fuentes:
    - PDFs procesados con OCR
    - Casos histÃ³ricos desde MySQL
    - Datos estructurados de APIs externas
    
    Args:
        fragments: Lista de dicts con {text, embedding, metadata}
        index_name: Nombre del Ã­ndice (ej: tariff_fragments)
    """
    client = get_os_client()
    embedder = GeminiEmbedder()
    
    # Generar embeddings en batch
    texts = [f["text"] for f in fragments]
    embeddings = embedder.embed_texts(texts)
    
    # Preparar documentos para bulk insert
    actions = []
    for fragment, embedding in zip(fragments, embeddings):
        doc = {
            "_index": index_name,
            "_source": {
                "text": fragment["text"],
                "embedding": embedding,
                "fragment_id": fragment.get("fragment_id"),
                "bucket": fragment.get("metadata", {}).get("bucket", "normativa"),
                "indexed_at": datetime.utcnow().isoformat()
            }
        }
        
        # Agregar metadata especÃ­fica segÃºn la fuente
        metadata = fragment.get("metadata", {})
        
        # Si viene de PDF (OCR)
        if "page" in fragment:
            doc["_source"]["page"] = fragment["page"]
            doc["_source"]["doc_id"] = fragment.get("doc_id")
        
        # Si viene de MySQL (histÃ³rico)
        if metadata.get("source") == "mysql":
            doc["_source"]["source"] = "mysql"
            doc["_source"]["table"] = metadata.get("table")
            doc["_source"]["record_id"] = metadata.get("record_id")
            doc["_source"]["hs_code"] = metadata.get("hs_code")
            doc["_source"]["product_name"] = metadata.get("product_name")
        
        # Campos comunes (capÃ­tulo, partida, subpartida)
        for field in ["chapter", "heading", "subheading"]:
            if field in fragment or field in metadata:
                doc["_source"][field] = fragment.get(field) or metadata.get(field)
        
        actions.append(doc)
    
    # Bulk insert
    success, failed = helpers.bulk(client, actions, raise_on_error=False)
    print(f"âœ… Indexados: {success} | âŒ Fallidos: {len(failed)}")
```

#### Paso 5: ~~Registro en MySQL~~ (No aplica - MySQL es fuente)

**Nota**: MySQL actÃºa como **fuente de datos**, no como destino. Los datos de MySQL se extraen mediante ETL y se indexan en OpenSearch junto con los documentos procesados vÃ­a OCR.

**Esquema tÃ­pico en MySQL (ejemplo)**:

```sql
-- Tabla de casos histÃ³ricos de clasificaciÃ³n
CREATE TABLE product_cases (
    id INT AUTO_INCREMENT PRIMARY KEY,
    product_name VARCHAR(500),
    description TEXT,
    hs_code VARCHAR(20),
    classification_date DATE,
    confidence_score FLOAT,
    classifier_name VARCHAR(100),
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_hs_code (hs_code),
    INDEX idx_date (classification_date)
);

-- Tabla de resoluciones aduaneras
CREATE TABLE customs_rulings (
    id INT AUTO_INCREMENT PRIMARY KEY,
    ruling_number VARCHAR(50) UNIQUE,
    product_description TEXT,
    hs_classification VARCHAR(20),
    legal_basis TEXT,
    ruling_date DATE,
    country_code CHAR(2),
    INDEX idx_ruling (ruling_number),
    INDEX idx_hs (hs_classification)
);

-- Tabla de productos con clasificaciÃ³n validada
CREATE TABLE validated_products (
    id INT AUTO_INCREMENT PRIMARY KEY,
    commercial_name VARCHAR(500),
    technical_specs TEXT,
    hs_code_6_digits VARCHAR(10),
    hs_code_full VARCHAR(20),
    validation_status ENUM('pending', 'approved', 'rejected'),
    validated_by VARCHAR(100),
    validated_at TIMESTAMP,
    INDEX idx_status (validation_status),
    INDEX idx_hs (hs_code_6_digits)
);
```

---

### 2. Consulta y ClasificaciÃ³n (RAG Pipeline)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Usuario  â”‚â”€â”€â”€â–¶â”‚  Hybrid Search   â”‚â”€â”€â”€â–¶â”‚ Re-rankingâ”‚â”€â”€â”€â–¶â”‚ LLM Gen â”‚
â”‚  (Query)  â”‚    â”‚ (OpenSearch kNN  â”‚    â”‚   (RRF)   â”‚    â”‚ (Gemini)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    + BM25)       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                 â”‚                  â”‚                          â”‚
                 â”‚ Busca en:        â”‚                          â”‚
                 â”‚ â€¢ PDFs normativa â”‚                          â”‚
                 â”‚ â€¢ Casos MySQL    â”‚                          â–¼
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                        â”‚ Respuestaâ”‚
                                                        â”‚+ CÃ³digo  â”‚
                                                        â”‚+ Fuentes â”‚
                                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

El flujo de bÃºsqueda recupera fragmentos de **ambas fuentes**:
1. **Documentos normativos** (PDFs procesados con OCR)
2. **Casos histÃ³ricos** (extraÃ­dos de MySQL)

OpenSearch fusiona resultados de ambas fuentes usando el mismo Ã­ndice vectorial.

#### Endpoint `/classify`

```python
# app/api.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="Tariff RAG API", version="1.0.0")

class ClassifyRequest(BaseModel):
    query: str
    top_k: int = 5
    min_confidence: float = 0.5

class ClassifyResponse(BaseModel):
    code: str
    description: str
    confidence: float
    evidence: List[str]
    metadata: dict

@app.post("/classify", response_model=ClassifyResponse)
async def classify_endpoint(req: ClassifyRequest):
    """
    Clasifica un producto en el Sistema Armonizado (HS).
    
    Flujo:
    1. BÃºsqueda hÃ­brida (semÃ¡ntica + lÃ©xica)
    2. Re-ranking con RRF
    3. GeneraciÃ³n de cÃ³digo con LLM
    4. BÃºsqueda de evidencia adicional
    5. ValidaciÃ³n de confianza
    """
    try:
        # 1. Retrieval
        os_client = get_os_client()
        index_name = get_settings().opensearch_index
        
        hits = hybrid_search_with_fallback(
            os_client=os_client,
            index=index_name,
            query_text=req.query,
            k=req.top_k
        )
        
        if not hits:
            raise HTTPException(
                status_code=404,
                detail="No se encontraron fragmentos relevantes"
            )
        
        # 2. Re-ranking (opcional)
        hits = rerank_hits(hits, req.query)
        
        # 3. GeneraciÃ³n con LLM
        result = generate_label(
            query=req.query,
            context=hits,
            min_confidence=req.min_confidence
        )
        
        # 4. Evidencia adicional para el cÃ³digo generado
        support_hits = retrieve_support_for_code(
            os_client=os_client,
            index_name=index_name,
            code=result.code,
            k=3
        )
        
        # 5. Construir respuesta
        evidence = [h["_source"]["text"] for h in hits[:3]]
        evidence.extend([s["text"] for s in support_hits])
        
        return ClassifyResponse(
            code=result.code,
            description=result.description,
            confidence=result.confidence,
            evidence=evidence,
            metadata={
                "num_hits": len(hits),
                "support_docs": len(support_hits),
                "strategy": "hybrid_with_llm"
            }
        )
        
    except Exception as e:
        logger.error(f"Error in /classify: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
```

#### BÃºsqueda HÃ­brida

```python
# app/os_retrieval.py
def hybrid_search_with_fallback(
    os_client: OpenSearch,
    index: str,
    query_text: str,
    k: int = 5
) -> List[dict]:
    """
    BÃºsqueda hÃ­brida con fallback automÃ¡tico.
    
    Estrategia:
    1. Intenta kNN semÃ¡ntico (embeddings)
    2. Si falla o vacÃ­o, usa BM25 lÃ©xico con boosts
    
    Returns:
        Lista de hits de OpenSearch con _score y _source
    """
    # Intento 1: kNN SemÃ¡ntico
    try:
        hits = knn_semantic_search(os_client, index, query_text, k)
        if hits and len(hits) >= 3:  # Umbral mÃ­nimo
            return hits
    except Exception as e:
        logger.warning(f"kNN search failed: {e}, falling back to BM25")
    
    # Fallback: BM25 con boosts de dominio
    return bm25_search(os_client, index, query_text, k)


def knn_semantic_search(
    os_client: OpenSearch,
    index: str,
    query_text: str,
    k: int = 5
) -> List[dict]:
    """
    BÃºsqueda semÃ¡ntica usando embeddings y kNN.
    
    OpenSearch query DSL:
    {
      "query": {
        "knn": {
          "embedding": {
            "vector": [0.1, 0.2, ...],  # 768 dims
            "k": 5
          }
        }
      }
    }
    """
    embedder = GeminiEmbedder()
    query_vector = embedder.embed_texts([query_text])[0]
    
    body = {
        "size": k,
        "query": {
            "knn": {
                "embedding": {
                    "vector": query_vector,
                    "k": k
                }
            }
        },
        "_source": [
            "fragment_id", "text", "bucket", "unit",
            "doc_id", "chapter", "heading", "subheading"
        ]
    }
    
    response = os_client.search(index=index, body=body)
    return response["hits"]["hits"]


def bm25_search(
    os_client: OpenSearch,
    index: str,
    query_text: str,
    k: int = 5
) -> List[dict]:
    """
    BÃºsqueda lÃ©xica BM25 con boosts a tÃ©rminos clave.
    
    HeurÃ­sticas:
    - Match phrase en cÃ³digos HS (boost 6.0)
    - Match en tÃ©rminos de dominio (boost 2.0)
    - DetecciÃ³n de cÃ³digos en query con regex
    """
    # TÃ©rminos de dominio con boost
    domain_terms = [
        "neumÃ¡tico", "neumÃ¡ticos", "llanta", "llantas",
        "caucho", "rubber", "pneumatic", "tyre", "tyres"
    ]
    
    should_clauses = [
        {"match": {"text": {"query": query_text, "boost": 3.0}}}
    ]
    
    # Agregar tÃ©rminos de dominio
    for term in domain_terms:
        should_clauses.append({
            "match": {"text": {"query": term, "boost": 2.0}}
        })
    
    # Detectar cÃ³digos HS en la query (ej: 4011.10)
    import re
    code_match = re.search(r"\b(\d{4})(?:\.(\d{2}))?(?:\.(\d{2}))?\b", query_text)
    if code_match:
        code = ".".join(filter(None, code_match.groups()))
        heading = code.split(".")[0]
        
        # Variantes del cÃ³digo
        variants = _generate_code_variants(code)
        for variant in variants:
            should_clauses.append({
                "match_phrase": {"text": {"query": variant, "boost": 6.0}}
            })
        
        # Boost al heading (4 dÃ­gitos)
        should_clauses.append({
            "match_phrase": {"text": {"query": heading, "boost": 4.0}}
        })
    
    body = {
        "size": k,
        "query": {
            "bool": {
                "should": should_clauses,
                "minimum_should_match": 1
            }
        },
        "_source": [
            "fragment_id", "text", "bucket", "unit",
            "doc_id", "chapter", "heading", "subheading"
        ]
    }
    
    response = os_client.search(index=index, body=body)
    return response["hits"]["hits"]


def _generate_code_variants(code: str) -> List[str]:
    """
    Genera variantes de escritura de cÃ³digos HS.
    
    Input: "4011.10"
    Output: ["4011.10", "401110", "4011 10", "4011-10", ...]
    """
    no_dot = code.replace(".", "")
    with_space = code.replace(".", " ")
    with_dash = code.replace(".", "-")
    return list(set([code, no_dot, with_space, with_dash]))
```

#### Re-ranking con RRF

```python
# app/chain_rag.py
def rerank_hits(hits: List[dict], query: str) -> List[dict]:
    """
    Re-ranking usando Reciprocal Rank Fusion (RRF).
    
    Formula RRF: score = Î£ 1 / (k + rank_i)
    donde k = 60 (constante estÃ¡ndar)
    
    Combina scores de mÃºltiples retrieval strategies.
    """
    k = 60  # Constante de RRF
    
    for i, hit in enumerate(hits):
        rank = i + 1
        hit["_rrf_score"] = 1.0 / (k + rank)
        
        # Opcional: boost por coincidencia de keywords
        text_lower = hit["_source"]["text"].lower()
        keyword_boost = 0.0
        
        keywords = ["neumÃ¡tico", "caucho", "pneumatic", "tire"]
        for kw in keywords:
            if kw in text_lower:
                keyword_boost += 0.1
        
        hit["_rrf_score"] += keyword_boost
    
    # Ordenar por RRF score
    hits.sort(key=lambda x: x["_rrf_score"], reverse=True)
    return hits
```

#### GeneraciÃ³n con LLM

```python
# app/generator_gemini.py
import google.generativeai as genai
from dataclasses import dataclass
import re

@dataclass
class ClassificationResult:
    code: str
    description: str
    confidence: float
    reasoning: str

def generate_label(
    query: str,
    context: List[dict],
    min_confidence: float = 0.5
) -> ClassificationResult:
    """
    Genera clasificaciÃ³n HS usando Gemini con contexto RAG.
    
    El contexto puede incluir:
    - Fragmentos de documentos normativos
    - Casos histÃ³ricos de clasificaciÃ³n desde MySQL
    - Ambos tipos mezclados por relevancia
    """
    # Construir contexto distinguiendo fuentes
    context_text = ""
    for i, hit in enumerate(context[:5], 1):
        src = hit["_source"]
        score = hit.get("_score", 0)
        
        # Identificar fuente
        source_label = "ğŸ“„ Normativa" if src.get("bucket") == "normativa" else "ğŸ“Š Caso HistÃ³rico"
        
        context_text += f"\n[{source_label} {i}] (relevancia: {score:.2f})\n"
        context_text += f"{src['text']}\n"
        
        # Metadata especÃ­fica de MySQL
        if src.get("source") == "mysql":
            if src.get("product_name"):
                context_text += f"  Producto: {src['product_name']}\n"
            if src.get("hs_code"):
                context_text += f"  Clasificado como: {src['hs_code']}\n"
            if src.get("confidence_score"):
                context_text += f"  Confianza previa: {src['confidence_score']:.2f}\n"
        
        # Metadata de documentos PDF
        if "chapter" in src and src["chapter"]:
            context_text += f"  CapÃ­tulo: {src['chapter']}\n"
        if "heading" in src and src["heading"]:
            context_text += f"  Partida: {src['heading']}\n"
    
    prompt = f"""Eres un experto en clasificaciÃ³n arancelaria segÃºn el Sistema Armonizado (HS).

Tu tarea es clasificar el siguiente producto en el cÃ³digo HS mÃ¡s especÃ­fico posible.

CONTEXTO DISPONIBLE (normativa y casos histÃ³ricos):
{context_text}

PRODUCTO A CLASIFICAR:
{query}

INSTRUCCIONES:
1. Analiza el producto y el contexto proporcionado
2. Considera tanto la normativa oficial como los casos histÃ³ricos similares
3. Si hay casos histÃ³ricos relevantes, Ãºsalos como referencia pero valida contra la normativa
4. Identifica el capÃ­tulo, partida, subpartida y cÃ³digo completo
5. Explica tu razonamiento paso a paso
6. Proporciona un nivel de confianza (0-1)

FORMATO DE RESPUESTA (obligatorio):
CÃ“DIGO: XXXX.XX.XX.XX
DESCRIPCIÃ“N: [descripciÃ³n tÃ©cnica del producto]
CONFIANZA: [nÃºmero entre 0 y 1]
RAZONAMIENTO: [justificaciÃ³n detallada citando los documentos]

RESPUESTA:"""

    # Llamada a Gemini
    model = genai.GenerativeModel("gemini-pro")
    
    generation_config = {
        "temperature": 0.3,  # Baja temperatura para mayor precisiÃ³n
        "top_p": 0.95,
        "top_k": 40,
        "max_output_tokens": 1024,
    }
    
    response = model.generate_content(
        prompt,
        generation_config=generation_config
    )
    
    # Parsear respuesta estructurada
    text = response.text
    
    code = _extract_field(text, "CÃ“DIGO:")
    description = _extract_field(text, "DESCRIPCIÃ“N:")
    confidence_str = _extract_field(text, "CONFIANZA:")
    reasoning = _extract_field(text, "RAZONAMIENTO:")
    
    # Validar y normalizar cÃ³digo
    code = _normalize_hs_code(code)
    
    # Parsear confianza
    try:
        confidence = float(confidence_str)
    except:
        confidence = 0.5
    
    # Validar confianza mÃ­nima
    if confidence < min_confidence:
        raise ValueError(
            f"Confianza {confidence:.2f} por debajo del mÃ­nimo {min_confidence}"
        )
    
    return ClassificationResult(
        code=code,
        description=description,
        confidence=confidence,
        reasoning=reasoning
    )


def _extract_field(text: str, field_name: str) -> str:
    """Extrae valor de un campo del formato estructurado."""
    pattern = f"{field_name}\\s*(.+?)(?=\\n[A-Z]+:|$)"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip() if match else ""


def _normalize_hs_code(code: str) -> str:
    """
    Normaliza cÃ³digo HS al formato estÃ¡ndar XXXX.XX.XX.XX
    
    Ejemplos:
    - "4011.10" -> "4011.10.00.00"
    - "401110" -> "4011.10.00.00"
    - "4011 10" -> "4011.10.00.00"
    """
    # Remover espacios y caracteres no numÃ©ricos excepto puntos
    clean = re.sub(r"[^\d.]", "", code)
    
    # Extraer dÃ­gitos
    digits = re.findall(r"\d+", clean)
    if not digits:
        return "0000.00.00.00"
    
    # Reconstruir en formato estÃ¡ndar
    parts = "".join(digits)
    
    if len(parts) < 4:
        parts = parts.ljust(4, "0")
    
    # Formato: XXXX.XX.XX.XX
    formatted = f"{parts[:4]}.{parts[4:6] if len(parts)>4 else '00'}"
    formatted += f".{parts[6:8] if len(parts)>6 else '00'}"
    formatted += f".{parts[8:10] if len(parts)>8 else '00'}"
    
    return formatted
```

---

## MÃ³dulos del Backend

### Estructura de Directorios

```
d:\MAESTRIA\tariff-rag/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api.py                    # Endpoints FastAPI
â”‚   â”œâ”€â”€ config.py                 # ConfiguraciÃ³n (Pydantic Settings)
â”‚   â”œâ”€â”€ metrics.py                # Prometheus metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ os_index.py               # Cliente OpenSearch
â”‚   â”œâ”€â”€ os_retrieval.py           # BÃºsqueda hÃ­brida
â”‚   â”œâ”€â”€ os_ingest.py              # IndexaciÃ³n bulk
â”‚   â”‚
â”‚   â”œâ”€â”€ embedder_gemini.py        # GeneraciÃ³n de embeddings
â”‚   â”œâ”€â”€ generator_gemini.py       # GeneraciÃ³n con LLM
â”‚   â”œâ”€â”€ chain_rag.py              # OrquestaciÃ³n RAG
â”‚   â”‚
â”‚   â”œâ”€â”€ ocr/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py               # Interfaces abstractas
â”‚   â”‚   â”œâ”€â”€ azure_provider.py    # Azure Form Recognizer
â”‚   â”‚   â””â”€â”€ tesseract_provider.py # Tesseract OCR
â”‚   â”‚
â”‚   â””â”€â”€ etl_mysql.py              # â­ ETL desde MySQL (FUENTE)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_opensearch.py        # Crear Ã­ndice
â”‚   â”œâ”€â”€ ingest_pdf.py             # Ingestar PDF
â”‚   â”œâ”€â”€ ingest_mysql.py           # â­ Ingestar desde MySQL
â”‚   â”œâ”€â”€ ingest_jsonl.py           # â­ Ingestar desde archivos JSONL
â”‚   â”œâ”€â”€ chunk_and_index.py        # FragmentaciÃ³n e indexaciÃ³n
â”‚   â”œâ”€â”€ sync_all.py               # Sincronizar todas las fuentes
â”‚   â””â”€â”€ evaluate.py               # EvaluaciÃ³n de mÃ©tricas
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ gradio_app.py             # Interfaz Gradio
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â”œâ”€â”€ test_generation.py
â”‚   â”œâ”€â”€ test_etl_mysql.py         # Tests de ETL
â”‚   â””â”€â”€ test_e2e.py
â”‚
â”œâ”€â”€ data/                         # Documentos fuente (PDFs)
â”œâ”€â”€ storage/                      # Datos persistentes
â”‚   â””â”€â”€ os/                       # OpenSearch data
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements.ui.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

### Script de Ingesta desde MySQL

```python
# scripts/ingest_mysql.py
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from app.etl_mysql import extract_mysql_fragments
from app.os_ingest import bulk_ingest_fragments
from app.config import get_settings

def main():
    """
    Script para ingestar datos desde MySQL hacia OpenSearch.
    
    Variables de entorno opcionales:
    - MYSQL_TABLE: tabla a procesar (default: product_cases)
    - MYSQL_TEXT_COL: columna con texto (default: description)
    - MYSQL_ID_COL: columna ID (default: id)
    
    Uso:
        docker exec rag-api python scripts/ingest_mysql.py
        
    O con variables:
        docker exec -e MYSQL_TABLE=customs_rulings rag-api python scripts/ingest_mysql.py
    """
    s = get_settings()
    table = os.environ.get("MYSQL_TABLE", "product_cases")
    text_col = os.environ.get("MYSQL_TEXT_COL", "description")
    id_col = os.environ.get("MYSQL_ID_COL", "id")
    
    print(f"ğŸ“¥ Extrayendo desde MySQL: {table}.{text_col}")
    
    fragments = extract_mysql_fragments(table, text_col, id_col)
    
    if fragments:
        print(f"âœ¨ Generando embeddings para {len(fragments)} fragmentos...")
        fragments_dict = [f.model_dump() for f in fragments]
        bulk_ingest_fragments(fragments_dict, s.opensearch_index)
        print(f"âœ… Ingestados {len(fragments)} fragmentos desde MySQL â†’ OpenSearch")
    else:
        print("â„¹ï¸ No se encontraron registros con texto en la tabla especificada")

if __name__ == "__main__":
    main()
```

### Script de Ingesta desde JSONL

```python
# scripts/ingest_jsonl.py
import json
from pathlib import Path

def ingest_jsonl(file_path: str, text_field: str = "text", bucket: str = "jsonl_import"):
    """
    Procesa e indexa un archivo JSONL.
    
    Formato esperado:
    {"text": "NeumÃ¡ticos radiales...", "hs_code": "4011.10", "product_name": "..."}    
    """
    from app.os_ingest import bulk_ingest_fragments
    from app.embedder_gemini import GeminiEmbedder
    
    embedder = GeminiEmbedder()
    
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            
            # Cargar objeto JSON
            obj = json.loads(line)
            
            # Mapeo bÃ¡sico
            text = obj.get(text_field, "")
            hs_code = obj.get("hs_code", "").strip()
            product_name = obj.get("product_name", "").strip()
            
            # Filtrar por tamaÃ±o mÃ­nimo
            if len(text) < 20:
                continue
            
            # Generar embedding
            embedding = embedder.embed_texts([text])[0]
            
            # Documento para indexar
            doc = {
                "_index": "tariff_fragments",
                "_source": {
                    "text": text,
                    "embedding": embedding,
                    "fragment_id": obj.get("id"),
                    "bucket": bucket,
                    "source": obj.get("metadata", {}).get("source", ""),
                    "doc_id": obj.get("metadata", {}).get("doc_id", ""),
                    "page": obj.get("metadata", {}).get("page_num", 1),
                    "role": obj.get("metadata", {}).get("role", ""),
                    "kind": obj.get("metadata", {}).get("kind", ""),
                    "indexed_at": datetime.utcnow().isoformat()
                }
            }
            
            # Indexar documento
            bulk_ingest_fragments([doc], "tariff_fragments")
            print(f"âœ… Ingestado fragmento: {obj.get('id')}")
```

### Ingesta desde JSONL (mapeo y filtros)

- Mapeo de campos:
  - id â†’ fragment_id
  - text â†’ text
  - metadata.doc_id â†’ doc_id
  - metadata.source â†’ source (archivo original)
  - metadata.page_num â†’ page
  - metadata.role â†’ role (p.ej. pageHeader, paragraph)
  - metadata.kind â†’ kind (p.ej. paragraph)
  - Se preserva metadata completa en _source.metadata; bucket=jsonl_import

- Filtros:
  - --include-types text table figure
  - --include-roles paragraph heading
  - --exclude-roles pageHeader pageFooter  (default)
  - --min-chars 20

Ejemplo (entrada JSONL):
{"id":"9483d9ca7628f38a_p0","type":"text","text":"VICEMINISTERIO DE POLÃTICA TRIBUTARIA","metadata":{"doc_id":"9483d9ca7628f38a","source":"Arancel_Boliviano_Parte_5.pdf","apiVersion":"2024-11-30","modelId":"prebuilt-layout","role":"pageHeader","page_num":1,"index":0,"span":{"offset":0,"length":37},"kind":"paragraph"}}

Comando:
docker exec rag-api python scripts/ingest_jsonl.py data/archivo.jsonl --exclude-roles pageHeader pageFooter --bucket normativa_jsonl
```
